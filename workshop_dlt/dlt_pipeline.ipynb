{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline quick_start load step completed in 0.44 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset quick_start_data\n",
      "The duckdb destination used duckdb:////workspaces/zoomcamp-data-engineer-2024/workshop_dlt/quick_start.duckdb location to store data\n",
      "Load package 1707716032.2552636 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# QUICK START\n",
    "\n",
    "data = [{'id': 1, 'name': 'Alice'},\n",
    "        {'id': 2, 'name': 'Bob'},\n",
    "        {'id': 3, 'name': 'Charlie'}]\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    dataset_name='quick_start_data',\n",
    "    pipeline_name='quick_start',\n",
    "    destination='duckdb'\n",
    ")\n",
    "\n",
    "load_data = pipeline.run(data=data, table_name='users')\n",
    "\n",
    "print(load_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA FROM API\n",
    "> Retrieve and load data from the GitHub API into DuckDB. Specifically, we will load issues from our dlt-hub/dlt repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline github_issues load step completed in 0.70 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset github_issues_data\n",
      "The duckdb destination used duckdb:////workspaces/zoomcamp-data-engineer-2024/workshop_dlt/github_issues.duckdb location to store data\n",
      "Load package 1707716033.6118846 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# BASIC LOAD\n",
    "\n",
    "url = \"https://api.github.com/repos/dlt-hub/dlt/issues\"\n",
    "\n",
    "# Make a request and check if it was successful\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='github_issues',\n",
    "    destination='duckdb',\n",
    "    dataset_name='github_issues_data'\n",
    ")\n",
    "\n",
    "load_data = pipeline.run(\n",
    "    data=response.json(),\n",
    "    table_name='github_issues',\n",
    "    write_disposition='replace'\n",
    ")\n",
    "\n",
    "print(load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row counts => Normalized data for the following tables:\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "- github_issues: 3 row(s)\n",
      "\n",
      "Load package 1707716035.0498624 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "=============================\n",
      "load_data_info => Pipeline github_issues_incremental load step completed in 0.69 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset github_issues_incremental_data\n",
      "The duckdb destination used duckdb:////workspaces/zoomcamp-data-engineer-2024/workshop_dlt/github_issues_incremental.duckdb location to store data\n",
      "Load package 1707716035.0498624 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# INCREMENTAL LOAD\n",
    "\n",
    "# We can pass a generator to the run method directly or use the @dlt.resource decorator to turn the generator into a dlt resource. The decorator allows you to specify the loading behavior and relevant resource parameters.\n",
    "\n",
    "# Let's improve our GitHub API example and get only issues that were created since last load. Instead of using replace write disposition and downloading all issues each time the pipeline is run. We a python generator\n",
    "\n",
    "@dlt.resource(table_name='github_issues', write_disposition='append')\n",
    "def get_issues(\n",
    "    created_at=dlt.sources.incremental(\n",
    "        'created_at', initial_value=\"1970-01-01T00:00:00Z\")\n",
    "):\n",
    "    # NOTE: we read only open issues to minimize number of calls to the API.\n",
    "    # There's a limit of ~50 calls for not authenticated Github users.\n",
    "    url = (\n",
    "        \"https://api.github.com/repos/dlt-hub/dlt/issues\"\n",
    "        \"?per_page=100&sort=created&directions=desc&state=open\"\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        yield res.json()\n",
    "\n",
    "        # Stop requesting pages if the last element was already\n",
    "        # older than initial value\n",
    "        # Note: incremental will skip those items anyway, we just\n",
    "        # do not want to use the api limits\n",
    "        if created_at.start_out_of_range:\n",
    "            break\n",
    "\n",
    "        # Next page\n",
    "        if 'next' not in res.links:\n",
    "            break\n",
    "\n",
    "        print(f\">>>>>>>>>>>>>Next page: {res.links}\")\n",
    "\n",
    "        url = res.links['next']['url']\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name='github_issues_incremental',\n",
    "    destination='duckdb',\n",
    "    dataset_name='github_issues_incremental_data'\n",
    ")\n",
    "\n",
    "load_data_info = pipeline.run(get_issues)\n",
    "row_counts = pipeline.last_trace.last_normalize_info\n",
    "\n",
    "print(f'row counts => {row_counts}')\n",
    "print('=============================')\n",
    "print(f'load_data_info => {load_data_info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row counts => Normalized data for the following tables:\n",
      "- _dlt_pipeline_state: 1 row(s)\n",
      "- github_issues: 2 row(s)\n",
      "\n",
      "Load package 1707716037.018522 is NORMALIZED and NOT YET LOADED to the destination and contains no failed jobs\n",
      "=============================\n",
      "load_data_info => Pipeline github_issues_merge load step completed in 0.99 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset github_issues_data_merge\n",
      "The duckdb destination used duckdb:////workspaces/zoomcamp-data-engineer-2024/workshop_dlt/github_issues_merge.duckdb location to store data\n",
      "Load package 1707716037.018522 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# UPDATE AND DEDUPLICATE DATA\n",
    "\n",
    "# The script above finds new issues and adds them to the database. It will ignore any updates to existing issue text, emoji reactions etc. To get always fresh content of all the issues you combine incremental load with merge write disposition, like in the script below.\n",
    "\n",
    "# We add primary_key argument to the dlt.resource() that tells dlt how to identify the issues in the database to find duplicates which content it will merge.\n",
    "\n",
    "# NOTE: we now track the updated_at field â€” so we filter in all issues updated since the last pipeline run (which also includes those newly created).\n",
    "\n",
    "# Pay attention how we use since parameter from GitHub API and updated_at.last_value to tell GitHub to return issues updated only after the date we pass. updated_at.last_value holds the last updated_at value from the previous run.\n",
    "\n",
    "@dlt.resource(\n",
    "    table_name=\"github_issues\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"id\",\n",
    ")\n",
    "def get_issues(\n",
    "    updated_at=dlt.sources.incremental(\n",
    "        \"updated_at\", initial_value=\"1970-01-01T00:00:00Z\")\n",
    "):\n",
    "    # NOTE: we read only open issues to minimize number of calls to\n",
    "    # the API. There's a limit of ~50 calls for not authenticated\n",
    "    # Github users\n",
    "    url = (\n",
    "        \"https://api.github.com/repos/dlt-hub/dlt/issues\"\n",
    "        f\"?since={updated_at.last_value}&per_page=100&sort=updated\"\n",
    "        \"&directions=desc&state=open\"\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        yield response.json()\n",
    "\n",
    "        # Get next page\n",
    "        if \"next\" not in response.links:\n",
    "            break\n",
    "        url = response.links[\"next\"][\"url\"]\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_issues_merge\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_issues_data_merge\",\n",
    ")\n",
    "\n",
    "load_data_info = pipeline.run(get_issues)\n",
    "row_counts = pipeline.last_trace.last_normalize_info\n",
    "\n",
    "print(f'row counts => {row_counts}')\n",
    "print('=============================')\n",
    "print(f'load_data_info => {load_data_info}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
